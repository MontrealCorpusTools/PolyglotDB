{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Adding extra information\n",
    "\n",
    "---\n",
    "#### Note\n",
    "\n",
    "In general, enrichment can be performed in any order (i.e., speaker enrichment is independent of syllable encoding),\n",
    "so you can perform the major sections in any order and the result\n",
    "is the same.  Within a section, however (i.e., [Encoding syllables](#Encoding-syllables)), the ordering of the steps is necessary (i.e., syllabic\n",
    "segments must be specified before syllables can be encoded).\n",
    "\n",
    "---\n",
    "\n",
    "First we begin with our standard imports and the path to the downloaded corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from polyglotdb import CorpusContext\n",
    "\n",
    "corpus_root = '/mnt/e/Data/pg_tutorial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding syllables\n",
    "\n",
    "To create syllables requires two steps.  The first is to specify the subset of phones in the corpus that are syllabic segments\n",
    "and function as syllabic nuclei.  In general these will be vowels, but can also include syllabic consonants.  Subsets in\n",
    "PolyglotDB are completely arbitrary sets of labels that speed up querying and allow for simpler references, see [Subset enrichment](https://polyglotdb.readthedocs.io/en/latest/enrichment_subset.html) for\n",
    "more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabics = [\"ER0\", \"IH2\", \"EH1\", \"AE0\", \"UH1\", \"AY2\", \"AW2\", \"UW1\", \"OY2\", \"OY1\", \"AO0\", \"AH2\", \"ER1\", \"AW1\",\n",
    "             \"OW0\", \"IY1\", \"IY2\", \"UW0\", \"AA1\", \"EY0\", \"AE1\", \"AA0\", \"OW1\", \"AW0\", \"AO1\", \"AO2\", \"IH0\", \"ER2\",\n",
    "             \"UW2\", \"IY0\", \"AE2\", \"AH0\", \"AH1\", \"UH2\", \"EH2\", \"UH0\", \"EY1\", \"AY0\", \"AY1\", \"EH0\", \"EY2\", \"AA2\",\n",
    "             \"OW2\", \"IH1\"]\n",
    "\n",
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_type_subset('phone', syllabics, 'syllabic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the syllabic segments have been marked as such in the phone inventory, the next step is to actually create the syllable\n",
    "annotations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_syllables(syllabic_label='syllabic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``encode_syllables`` function uses a maximum onset algorithm based on all existing word-initial sequences of phones not\n",
    "marked as ``syllabic`` in this case, and then maximizes onsets between syllabic segments.  As an example, something like\n",
    "``astringent`` would have a phone sequence of ``AH0 S T R IH1 N JH EH0 N T``.  In any reasonably-sized corpus of English, the\n",
    "list of possible onsets would in include ``S T R`` and ``JH``, but not ``N JH``, so the sequence would be syllabified as\n",
    "``AH0 . S T R IH1 N . JH EH0 N T``.\n",
    "\n",
    "---\n",
    "#### Note\n",
    "\n",
    "See [Creating syllable units](https://polyglotdb.readthedocs.io/en/latest/enrichment_syllables.html) for more details on syllable enrichment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding utterances\n",
    "\n",
    "As with syllables, encoding utterances consists of two steps.  The first is marking the \"words\" that are actually non-speech\n",
    "elements within the transcript.  When a corpus is first imported, every annotation is treated as speech.  As such, encoding\n",
    "labels like ``<SIL>`` as pause elements and not actual speech sounds is a crucial first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause_labels = ['<SIL>']\n",
    "\n",
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_pauses(pause_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pauses are encoded, the next step is to actually create the utterance annotations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_utterances(min_pause_length=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, it is desirable\n",
    "to not split groups of words for all pauses, i.e., small pauses might be inserted due to forced alignment, or can signify a\n",
    "smaller break than an utterance break.  Thus usually there is a minimum pause length to determine the breaks between utterances, as above.\n",
    "\n",
    "---\n",
    "#### Note\n",
    "\n",
    "See [Creating utterance units](https://polyglotdb.readthedocs.io/en/latest/enrichment_utterances.html) for more details on u enrichment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker enrichment\n",
    "\n",
    "Included in the tutorial corpus is a CSV containing speaker information, namely their gender and their actual name rather\n",
    "than the numeric code used in LibriSpeech.  This information can be imported into the corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_enrichment_path = os.path.join(corpus_root, 'enrichment_data', 'speaker_info.csv')\n",
    "\n",
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.enrich_speakers_from_csv(speaker_enrichment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once enrichment is complete, we can then query information and extract information about these characteristics of speakers.\n",
    "\n",
    "---\n",
    "#### Note\n",
    "\n",
    "See [Enrichment via CSV files](https://polyglotdb.readthedocs.io/en/latest/enrichment_csvs.html) for more details on enrichment from csvs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress enrichment\n",
    "\n",
    "---\n",
    "#### Important\n",
    "\n",
    "   Stress enrichment requires the [Encoding syllables](#Encoding-syllables) step has been completed.\n",
    "\n",
    "---\n",
    "\n",
    "Once syllables have been encoded, there are a couple of ways to encode the stress level of the syllable (i.e., primary stress,\n",
    "secondary stress, or unstressed).  The way used in this tutorial will use a lexical enrichment file included in the tutorial\n",
    "corpus.  This file has a field named ``stress_pattern`` that gives a pattern for the syllables based on the stress.  For\n",
    "example, ``astringent`` will have a stress pattern of ``0-1-0``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_enrichment_path = os.path.join(corpus_root, 'enrichment_data', 'iscan_lexicon.csv')\n",
    "\n",
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.enrich_lexicon_from_csv(lexicon_enrichment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_stress_from_word_property('stress_pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this enrichment step, words will have a type property of ``stress_pattern`` and syllables will have a token property\n",
    "of ``stress`` that can be queried on and extracted.\n",
    "\n",
    "---\n",
    "#### Note\n",
    "\n",
    "See [Encoding stress](https://polyglotdb.readthedocs.io/en/latest/enrichment_syllables.html#encoding-stress) for more details on enrichment from csvs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional enrichment\n",
    "\n",
    "---\n",
    "#### Important\n",
    "\n",
    "Speech rate enrichment requires that both the [Encoding syllables](#Encoding-syllables) and [Encoding utterances](#Encoding-utterances)\n",
    "   steps have been completed.\n",
    "\n",
    "---\n",
    "\n",
    "One of the final enrichment in this tutorial is to encode speech rate onto utterance annotations.  The speech rate measure used\n",
    "here is going to to be syllables per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_rate('utterance', 'syllable', 'speech_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will encode the number of syllables per word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CorpusContext('pg_tutorial') as c:\n",
    "    c.encode_count('word', 'syllable', 'num_syllables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the enrichments complete, a token property of ``speech_rate`` will be available for query and export on utterance\n",
    "annotations, as well as one for ``num_syllables`` on word tokens.\n",
    "\n",
    "---\n",
    "#### Note\n",
    "\n",
    "See [Hierarchical enrichment](https://polyglotdb.readthedocs.io/en/latest/enrichment_hierarchical.html) for more details on encoding properties based on the rate/count/position of lower\n",
    "   annotations (i.e., phones or syllables) within higher annotations (i.e., syllables, words, or utterances).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
